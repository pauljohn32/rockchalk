%% LyX 2.0.8.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[american,noae]{scrartcl}
\usepackage{lmodern}
\renewcommand{\sfdefault}{lmss}
\renewcommand{\ttdefault}{lmtt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\setlength{\parskip}{\smallskipamount}
\setlength{\parindent}{0pt}
\usepackage{babel}
\usepackage{url}
\usepackage[authoryear]{natbib}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=false]
 {hyperref}
\usepackage{breakurl}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
<<echo=F>>=
  if(exists(".orig.enc")) options(encoding = .orig.enc)
@
\usepackage{enumitem}		% customizable list environments
\newlength{\lyxlabelwidth}      % auxiliary length 
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\strong}[1]{\textbf{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\VignetteIndexEntry{Rchaeology}

\usepackage{Sweavel}
\usepackage{graphicx}
\usepackage{color}

\usepackage{babel}
\usepackage[samesize]{cancel}



\usepackage{ifthen}

\makeatletter

\renewenvironment{figure}[1][]{%

 \ifthenelse{\equal{#1}{}}{%

   \@float{figure}

 }{%

   \@float{figure}[#1]%

 }%

 \centering

}{%

 \end@float

}

\renewenvironment{table}[1][]{%

 \ifthenelse{\equal{#1}{}}{%

   \@float{table}

 }{%

   \@float{table}[#1]%

 }%

 \centering

%  \setlength{\@tempdima}{\abovecaptionskip}%

%  \setlength{\abovecaptionskip}{\belowcaptionskip}%

% \setlength{\belowcaptionskip}{\@tempdima}%

}{%

 \end@float

}


%\usepackage{listings}
% Make ordinary listings look as if they come from Sweave
\lstset{tabsize=2, breaklines=true,style=Rstyle}

% In document Latex options:
\fvset{listparameters={\setlength{\topsep}{0em}}}
\def\Sweavesize{\scriptsize} 
\def\Rcolor{\color{black}} 
\def\Rbackground{\color[gray]{0.90}}



\usepackage{babel}

\usepackage[samesize]{cancel}



\usepackage{ifthen}



\usepackage{listings}% Make ordinary listings look as if they come from Sweave
\lstset{tabsize=2, breaklines=true, style=Rstyle}

\makeatother

\begin{document}

\title{Rchaeology: Idioms of R Programming}


\author{Paul E. Johnson <pauljohn @ ku.edu>}

\maketitle
This document was initiated on May 31, 2012. The newest copy will
always be available at \url{http://pj.freefaculty.org/R} and as a
vignette in the R package ``rockchalk''.
\begin{description}
\item [{Rchaeology:}] The study of R programming by investigation of R
source code. It is the effort to discern the programming strategies,
idioms, and style of R programmers in order to better communicate
with them.
\item [{Rchaeologist:}] One who practices Rchaeology.
\end{description}
These are Rcheological observations about the style and mannerisms
of R programmers in their native habitats. Almost all of the insights
here are gathered from the r-help and r-devel emails lists, the stackoverflow
website pages for R, and the R source code itself. These are lessons
from the ``school of hard knocks.''

How is this different from Rtips(\url{http://pj.freefaculty.org/R/Rtips.{pdf,html}})?
\begin{enumerate}
\item This is oriented toward programming R, rather than using R.
\item It is more synthetic, aimed more at finding ``what's right'' rather
than ``what works.''
\item It is written with Sweave (using Harrell's Sweavel style) so that
code examples work.
\end{enumerate}
Where did the ``R Style'' section go? It was removed to a separate
vignette, RStyle, in the rockchalk package.

\tableofcontents{}

<<echo=F>>=
dir.create("plots", showWarnings=F)
@

% In document Latex options:
\fvset{listparameters={\setlength{\topsep}{0em}}}
\SweaveOpts{prefix.string=plots/plot,ae=F,height=4,width=6}

<<Roptions, echo=F>>=
options(width=100, continue="  ")
options(useFancyQuotes = FALSE) 
set.seed(12345)
pdf.options(onefile=F,family="Times",pointsize=12)
@


\section{Introduction: R Idioms.}

This vignette is about the R idioms I have learned while working on
the rockchalk package. At the current time, I don't understand all
of the R idioms that are common in the advanced R programmers' conversation,
but I am getting some traction.

There is a language gap between an R user and an R programmer. Users
write ``scripts'' that use functions from R packages. Users don't
write (many) functions. Users don't make packages. And many users
are happy to keep it that way. For users who want to become programmers,
there is usually a harsh awakening. R for development is a different
language. Well, that's wrong. It is a different dialect. 

A transitional R user will have to learn a lot of terminology and
undergo a change of paradigm. There are resources available! Everybody
should subscribe to the email lists for the R project (\url{http://www.r-project.org/mail.html}),
especially r-help (for user questions) and r-devel. Rchaeologically
speaking, that is the native habitat of the R programmers. There is
also a burgeoning collection of blogs and Web forums, perhaps most
notably the R section on StackExchange (\url{http://stackoverflow.com/questions/tagged/r}).
Excellent books have been published. As an Rcheologist, I am drawn
to the books that are written by the R insiders. \emph{S Programming},
by William Venables and Brian Ripley \citeyearpar{venables_s_2000},
is a classic. Books by pioneering developers Robert Gentleman, \emph{R
Programming for Bioinformatics} (2009), and John Chambers, \emph{Software
for Data Analysis} \citeyearpar{chambers_software_2008}, are, well,
awesome. In 2012, I started teaching R programming using Norman Matloff's,
\emph{The Art of R Programming} (2012), which I think is great and
recommend strongly (even though Professor Matloff is not one of the
R Core Team members, so far as I know). 

There are now three types of object-oriented programming in R (S3,
S4, and reference classes) and the programmer is apparently free to
select among them without prejudice. The best brief explanation of
S3 that I've found is in Friedrich Leisch's brief note about R packaging,
``Creating R Packages: A Tutorial'' (2009). One should be mindful
of the fact that R is provided with several manuals, one of which
is the \emph{R Language Definition}. I find that one difficult to
understand, and I usually can't understand it until I search through
the R source code and packages for usage examples.

I usually approach R coding in three phases. I write code that ``works'',
however tedious and slow it might be. There will be a lot of ``copy
and paste'' constructions with tedious, manual editing of commands.
Then I read through the code and look for repetitious elements, and
I re-organize to make functions that abstract that work. If there
are any stanzas that look mostly the same, except that ``x1'' is
repalced by ``x2'', I know the work is not done. Finally, I look
through the code to find ``dumb'' constructions that can be cleaned
up and completed with fewer lines. A dumb construction is one that
I would be ashamed to show to an expert R programmer.

Beginning R programmers, the ones who have never studied C or Java
or Fortran, will often become ``stuck'' in stage one of that process.
Experienced programmers with more formal training will usually have
the ``little voice'' in the back of their minds saying ``there's
a better way,'' and, unfortunately, R novices don't hear that voice.
Lacking some of the discipline imposed by those other languages, the
usual ``rambling'' R script will seem ``good enough.'' The first
step for the transitional R programmer, the one who does not want
to be a novice any more, is to take a harsh look at the code that
has already been written.

The rockchalk package has many functions that receive fitted regression
models and re-arrange their components for re-fitting or plotting.
A good deal of the advice I have to offer is about the process of
managing R formula, but I have also accumulated some lessons in argument
handling. 


\section{do.call(), eval(), substitute(), formula().}

If the transitional programmer understands these four functions, she/he
will be well on the way to escaping the category of novice R programmer.
The other benefit is that the code written by the experts, including
the R source code itself, will become much more understandable.


\subsection{Rewriting Formulas. My Introductory Puzzle.}

On May 29, 2012, I was working on a regression problem in the rockchalk
package. I have a number of functions that receive elementary regressions
and then change them. I need to receive a fitted model, extract the
formula, change some variables, and then revise the formula to match
the new variables. And then run the model over again. 

The functions meanCenter() and residualCenter() receive a fitted regression
model, transform some variables, and then fit a new regression. Suppose
a regression has been fitted as ``\code{y \textasciitilde{} x1{*}x2}''.
The R result will estimate a predictive formula such as $\hat{y}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}x1_{i}+\hat{\beta}_{2}x2_{i}++\hat{\beta}_{3}x1_{i}\cdot x2_{i}$.
meanCenter() will replace the non-centered variables ``x1'' and
``x2'' with mean-centered variables ``x1c'' and ``x2c''. The
original formula y \textasciitilde{} x1{*}x2 must be replaced with
y \textasciitilde{} x1c{*}x2c. I found this to be a very complicated
problem with a very satisfying answer. 

My first effort used R's update function. That is the most obvious
approach. I learned very quickly that update() is not sufficient.
It is fairly easy to replace x1 with x1c in the formula, but not when
x1 is logged or otherwise transformed. Here is the runable example
code that demonstrates the problem. 

<<include=T, results=hide>>=
dat <- data.frame(x1 = rnorm(100, m = 50), x2 = rnorm(100, m = 50),
  x3 = rnorm(100, m = 50), x4 = rnorm(100, m=50), y = rnorm(100))
m2 <- lm(y ~ log(x1) + x2*x3, data = dat)
suffixX <- function(fmla, x, s){
    upform <- as.formula(paste(". ~ .", "-", x, "+", paste(x, s, sep = ""), sep=""))
    update.formula(fmla, upform)
}
newFmla <- formula(m2)
newFmla
suffixX(newFmla, "x2", "c")
suffixX(newFmla, "x1", "c")
@

Run that and check the last few lines of the output. See how the update
misses x1 inside log(x1) or in the interaction?

\inputencoding{latin9}\begin{lstlisting}
> newFmla <- formula(m2)
> newFmla
y ~ log(x1) + x2 * x3
> suffixX(newFmla, "x2", "c")
y ~ log(x1) + x3 + x2c + x2:x3
> suffixX(newFmla, "x1", "c")
y ~ log(x1) + x2 + x3 + x1c + x2:x3
\end{lstlisting}
\inputencoding{utf8}

I asked the members of r-help for assistance. Lately I've had very
good luck with r-help. Gabor Grothendieck wrote an answer to r-help
on May 29, 2012. He said simply, ``Try substitute,'' with this example.

\inputencoding{latin9}\begin{lstlisting}
> do.call("substitute", list(newFmla, setNames(list(as.name("x1c")), "x1")))
y ~ log(x1c) + x2 * x3
\end{lstlisting}
\inputencoding{utf8}

Problem solved, in a single line. 

That's very clever, I think. It packs together a half-dozen very deep
thoughts. It has most of the essential secrets of R's guts, laid out
in a single line. It has do.call(), substitute(), it interprets a
formula as a list, and it shows that every command in R is, when it
comes down to brass tacks, a list. 

I would like to take up these separate pieces in order.


\subsection{A Formula Object is a List.}

While struggling with this, I noticed this really interesting pattern.
The solution depends on it. The object ``newFmla'' is not just a
text string. It prints out as if it were text, but it is actually
an R list object. Its parts can be probed recursively, to eventually
reveal all of the individual pieces:

<<newFla10>>=
newFmla
newFmla[[1]]
newFmla[[2]]
newFmla[[3]]
newFmla[[3]][[2]]
newFmla[[3]][[2]][[2]]
@

How could I put that information to use? Read on.


\subsection{do.call() and eval()}

In my early work as an Rchaeologist, I had noticed eval() and do.call(),
but did not understand their significance. Coming to grips with these
ideas is a critical step in the R programmer's growth, because they
separate the ``script writer'' from the ``language programmer.''
Whenever difficult problems arise in r-help, the answer almost invariably
involves do.call() or eval(). 


\subsubsection{do.call()}

Let's concentrate on do.call first. The syntax is like this

\inputencoding{latin9}\begin{lstlisting}
do.call("someRFunction", aListOfArgumentsToGoInTheParentheses)
\end{lstlisting}
\inputencoding{utf8}

It is as if we were telling R to run this:

\inputencoding{latin9}\begin{lstlisting}
someRFunction(aListOfArgumentsToGoInTheParentheses)
\end{lstlisting}
\inputencoding{utf8}

We use do.call() because it is much more flexible than calling someRFunction()
directly.

Let's consider an example that runs a regression the ordinary way,
and then with do.call. In this example, the role of ``someRFunction''
will be played by lm() and the list of arguments will be the parameters
of the regression. The regression m1 will be constructed the ordinary
way, while m2 is constructed with do.call().

<<>>=
m1 <- lm(y ~ x1*x2, data = dat)
coef(m1)
regargs <- list(formula = y ~ x1*x2, data = quote(dat))
m2 <- do.call("lm", regargs)
coef(m2)
all.equal(m1, m2)
@

The object regargs is a list of arguments that R can understand when
they are supplied to the lm function. 

do.call() is a powerful, mysterious symbol. It holds flexibility;
we can calculate ``on the language'' to create commands and then
run them. I first needed do.call() when we had a simulation project
that ran very slowly. There's a writeup in the working examples distributed
with rockchalk called stackListItems-01.R. I was using rbind() over
and over to join the results of simulation runs. Basically, the code
was like this

\inputencoding{latin9}\begin{lstlisting}
for (i in 1:10000){
   dat <- someHugeSimulation(i)
   result <- rbind(result, dat)
}
\end{lstlisting}
\inputencoding{utf8}

That will call rbind() 10000 times. I had not realized that rbind()
is time-consuming. It accesses a new chunk of memory each time it
is run. On the other hand, we could collect those results in a list,
then we can call rbind() one time to smash together all of the results.

\inputencoding{latin9}\begin{lstlisting}
for (i in 1:10000){
   mylist[[i]] <- someHugeSimulation(i)
}
result <- do.call("rbind", mylist)
\end{lstlisting}
\inputencoding{utf8}

It is much faster to run rbind() only once. It would be OK if we typed
it all out like this:

\inputencoding{latin9}\begin{lstlisting}
result <- rbind(mylist[[1]], mylist[[2]], mylist[[3]], ..., mylist[[10000])
\end{lstlisting}
\inputencoding{utf8}

But who wants to do all of that typing? How tiresome! Thanks to Erik
Iverson in r-help, I understand that

\inputencoding{latin9}\begin{lstlisting}
result <- do.call("rbind", mylist)
\end{lstlisting}
\inputencoding{utf8}

is doing the EXACT same thing. ``mylist'' is a list of arguments.
do.call is \emph{constructing} a function call from the list of arguments.
It is \emph{as if} I had actually typed rbind with 10000 arguments.

The beauty in this is that we could design a program that can assemble
the list of arguments, and also choose the function to be run, on
the fly. We are not required to literally write the function in quotes,
as in ``rbind''. We could instead have a variable that is calculated
to select one function among many, and then use do.call on that. In
a very real sense, we could write a program that can write itself
as it runs. 

From all of this (and a peek at ?call), I arrive at an Rchaeological
eureka! A call object is a quoted command plus a list of arguments
for that command. 


\subsubsection{eval()}

Where does eval() fit into the picture? do.call() manufactures a call
and executes it immediately. It is possible to arrest the process
mid stream, to capture the call without evaluating it with eval().
The terminology in the R documentation on this is difficult, mainly
because there are several different ways to create a piece of not-yet-evaluated
code. By \strong{not-yet-evaluated code}, I mean an ``unevaluated
expression'' or a ``call object'', anything that can be handled
by the eval() function. A not-yet-evaluated object holds some syntax
that can we need later. I think of do.call() as a contraction of ``eval''
and ``call''. As far as I can tell, do.call(\textquotedbl{}lm\textquotedbl{},
list(arguments)) is the same as eval(call(``rbind'', arguments)). 

There are many ways to ask R to create the not-yet-evaluated object,
that's one of the confusing things (in my opinion). Most obviously,
call() function can be used for that. But functions like quote() and
expression() can also create not-yet-evaluated code.

Everybody has experienced an error like this:

\inputencoding{latin9}\begin{lstlisting}
> x1 + x2
Error: object 'x1' not found
\end{lstlisting}
\inputencoding{utf8}

Whenever we use a variable, the R interpreter expects to find it.
We know those variables are in the data frame dat, but the interpreter
does not know that's what we mean. But we can ask R to trust us with
the quote() function, which, basically, means ``here's some code
we will evaluate later'':

\inputencoding{latin9}\begin{lstlisting}
> mycall <- quote(x1 + x2)
\end{lstlisting}
\inputencoding{utf8}

The object ``mycall'' is an R call object, something that might
be evaluated in the future. 

When we want to evaluate ``x1 + x2'', we ask R do to so:

\inputencoding{latin9}\begin{lstlisting}
eval(mycall, dat)
\end{lstlisting}
\inputencoding{utf8}The second argument supplies the place where x1 and x2 are to be found.
R users may never have noticed the eval() function, but most will
have used the with() function, which is simply a clever re-phrasing
of the eval function's interface. It does the same thing:\inputencoding{latin9}
\begin{lstlisting}
with(dat, x1 + x2)
\end{lstlisting}
\inputencoding{utf8}

and so does \inputencoding{latin9}
\begin{lstlisting}
with(dat, eval(mycall)).
\end{lstlisting}
\inputencoding{utf8}

Now, back to the regression example we were working on before. We
can create the call object using the call() function, which requires
the name of a function and a comma-separated list of quoted arguments.
I quote each of the bits of syntax in the arguments because I don't
want R to replace ``dat'' with the actual data frame, I only want
it to remember the name.

<<>>=
m3 <- lm(y ~ x1*x2, data = dat)
coef(m3)
mycall <- call("lm", quote(y ~ x1*x2), data = quote(dat))
m4 <- eval(mycall)
coef(m4)
@

The main reason for using eval is that we can ``piece together''
commands and then run them after we have assembled all the pieces.
The nickname for this seems to be ``computing on the language.''
There is a discussion of it in the R Language Definition. It is also
surveyed in Hadley Wickham's new book, \emph{Advanced R} \citeyearpar{Wickham2015}.

Did you ever notice that you get an error if you run ``x1 + x2'',
but if you run ``y $\sim$ x1 + x2'', then there is no error? It
is as if R is wrapping your code in protective tissue, something like
quote(). However, the protective wrapper in this case is actually
the function called formula(). The R interpreter notices the symbol
$\sim$ in the syntax, and so it assumes you meant to run

\inputencoding{latin9}\begin{lstlisting}
formula(y ~ x1 + x2)
\end{lstlisting}
\inputencoding{utf8}

A formula is another kind of not-yet-evaluated thing. It is something
that we can fiddle with before evaluating. 

Create a complicated formula. Note that the lm function receives that
formula object without trouble.

<<>>=
f1 <- y ~ x1 + x2 + x3 + log(x4)
class(f1)
m5 <- lm(f1, data = dat)
coef(m5)
@

The object f1 is a formula object. Its not just a text string. Observe
it has separate pieces, just like newFmla in the example problem that
started this section.

<<>>=
f1[[1]]
f1[[2]]
f1[[3]]
f1[[3]][[1]]
f1[[3]][[2]]
f1[[3]][[3]]
@\\
Note that f1 created in this way must be a syntactically valid R formula;
it cannot include any other regression options.

\inputencoding{latin9}\begin{lstlisting}
> f1 <- y ~ x1 + x2 + x3 + log(x4), data=dat 
Error: unexpected ',' in "f1 <- y ~ x1 + x2 + x3 + log(x4),"
\end{lstlisting}
\inputencoding{utf8}

If I declare f1exp as an expression, then R does not re-interpret
it as a formula (f1exp is an unevaluated expression, the R parser
has not translated it yet). To use that as a formula in the regression,
we have to evaluate it.

<<>>=
f1exp <- expression(y ~ x1 + x2 + x3 + log(x4))
class(f1exp)
m6 <- lm(eval(f1exp), data = dat)
@

When f1exp is evaluated, what do we have? Here's the answer.

<<>>=
f1expeval <- eval(f1exp)
class(f1expeval)
all.equal(f1expeval, f1)
m7 <- lm(f1expeval, data=dat)
all.equal(coef(m5), coef(m6), coef(m7))	
@

The point here is that the pieces of an ordinary use command can be
separated and put back together again before the work of doing calculations
begins. We can edit the formula. Suppose we replace a part:

<<>>=
f1[[3]][[2]] <- quote(x1 + log(x2))
m8 <- lm(f1, data = dat)
coef(m8)
@

Now we turn back to the main theme. How is eval() used in functions?
Some functions take a lot of arguments. We may want to do a lot of
fine tuning on the arguments, before they are assembled and evaluated.
Hence, it is very important that we can keep some bits of R that are
symbolic, rather than calculated, until we need them. 

If you are looking for examples of eval() in the R source code, there
is a nice one in the beginning of the lm() function. Suppose a user
submits a command like ``lm(y \textasciitilde{} x, data = dat, x
= TRUE, y = TRUE).'' Inside lm(), the import is re-organized. Here
are the first lines of the lm() function

\inputencoding{latin9}\begin{lstlisting}[breaklines=true,numbers=left,numberstyle={\footnotesize},tabsize=4]
lm <- function (formula, data, subset, weights, na.action, method = "qr",
    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE,
    contrasts = NULL, offset, ...)
{
    ret.x <- x
    ret.y <- y
    cl <- match.call()
    mf <- match.call(expand.dots = FALSE)
    m <- match(c("formula", "data", "subset", "weights", "na.action",
        "offset"), names(mf), 0L)
    mf <- mf[c(1L, m)]
    mf$drop.unused.levels <- TRUE
    mf[[1L]] <- as.name("model.frame")
    mf <- eval(mf, parent.frame())
\end{lstlisting}
\inputencoding{utf8}

Now suppose we turn on the debugger and run a regression in R with
a command like this.

\inputencoding{latin9}\begin{lstlisting}
debug(lm)
m1 <- lm(y ~ x1 * x2, data = dat, x = TRUE, y = TRUE)
\end{lstlisting}
\inputencoding{utf8}

After that, we are ``in'' the function, stepping through line-by-line.
In line 8, the match.call() function is used to grab a copy of the
command that I typed. We can see that mf is exactly the same as my
command, except R has named the arguments:

\inputencoding{latin9}\begin{lstlisting}
> mf
lm(formula = y ~ x1 * x2, data = dat, x = TRUE, y = TRUE)
\end{lstlisting}
\inputencoding{utf8}

That's not just a string of letters, however. It is a call object,
a list with individual pieces that can be revised. Recall that the
first element in a call object is the name of a function, and the
following elements are the arguments. Lines 10 and 11 check the names
of mf for the presence of certain arguments, and throw away the rest.
It only wants the arguments we would be needed to run the function
model.frame. Line 12 adds an argument to the list, drop.unused.levels.
Up to that point, then, we can look at the individual pieces of mf:

\inputencoding{latin9}\begin{lstlisting}
> names(mf) 
[1] ""      "formula"       "data" [4] "drop.unused.levels"
> mf[[1]]
lm
> mf[[2]]
y ~ x1 * x2
> mf[[3]]
dat
> mf[[4]] 
[1] TRUE
\end{lstlisting}
\inputencoding{utf8}

The object mf has separate pieces that can be revised and then evaluated.
Line 13 replaces the element 1 in mf with the symbol ``model.frame''.
That's the function that will be called. Line 14 is the coup de grâce,
when the revised call ``mf'' is sent to eval. In the end, it is
\emph{as if} lm had directly submitted the command

\inputencoding{latin9}\begin{lstlisting}
mf <- model.frame(y ~ x1 * x2, data = dat, drop.unused.levels = TRUE)
\end{lstlisting}
\inputencoding{utf8}

It would not do to simply write that into the lm function, however,
because some people use variables that have names different from y,
x1, and x2, and their data objects may not be called dat. lm allows
users to input whatever they want for a formula and data, and then
lm takes what it needs to build a model frame.


\subsection{substitute()}

Most R users I know have not used substitute, except possibly if they
try to use plotmath. In the context of plotmath, the problem is as
follows. Plotmath causes the R plot functions to convert expressions
into mathematical symbols in a way that is reminiscent of \LaTeX{}.
For example, a command like this:

\inputencoding{latin9}\begin{lstlisting}
text(4, 4, expression(gamma))
\end{lstlisting}
\inputencoding{utf8}will draw the gamma symbol at the position (4,4). We can use paste
to combine symbolic commands and text like so:

\inputencoding{latin9}\begin{lstlisting}
text(4, 4, expression(paste(gamma, " = 7")))
\end{lstlisting}
\inputencoding{utf8}The number 7 is a nice number, but what if we want to calculate something
and insert it into the expression? Your first guess might be to insert
a function that makes a calculation, such as the mean, but this fails:

\inputencoding{latin9}\begin{lstlisting}
text(4, 4, expression(paste(gamma, mean(x))))
\end{lstlisting}
\inputencoding{utf8}In order to smuggle the result of a calculation into an expression,
some fancy footwork is required. In the help page for plotmath, examples
using the functions bquote and substitute are offered. I have found
this to be quite frustrating and difficult to do, and if one examines
my collection of examples on \url{http://pj.freefaculty.org/R/WorkingExamples},
one will find quite a few plotmath exercises.

For the particular purpose of blending expressions with calculation
results, I find the bquote function to be more easily understandable.
However, when going beyond plotmath, I expect most R programmers will
need to use substitute() instead, so I will discuss that in this section.
The plotmath help page points to syntax like this:

<<fig=T, height=3, width=4>>=
plot(1:10, seq(1,5, length.out=10), type = "n", main="Illustrating Substitute with plotmath", xlab="x", ylab="y")
text(5, 4, substitute(gamma + x1mean, list(x1mean = mean(dat$x1))))
text(5, 2, expression(paste(gamma, " plus the mean of x1")))
@

Run ?substitute and one is brought to a famous piece of Rchaeological
pottery: 
\begin{quote}
‘substitute’ returns the parse tree for the (unevaluated) expression
‘expr’, substituting any variables bound in ‘env’. 
\end{quote}
Pardon me. parse tree? We've seen expressions already, that part is
not so off putting. But ``parse tree''? Really?

This is one of those points at which being an Rchaeologist has real
benefits. We have to dig deeper, to try to understand not only what
the R programmer says, but what she is actually trying to do. The
manual page gives us some insights into the R programmer, and it is
his or her view of his or her own actions, but it doesn't necessarily
speak to how we should understand substitute(). 

For me, the only workable approach is to build up a sequence of increasingly
complicated examples. I start by creating the list of replacements.
This replacement list can have a format like this: 

<<>>=
sublist <- list(x1 = "alphabet", x2 = "zoology")
@

Suppose I have some other object in which x1 and x2 need to be replaced.
Specifically, when an expression has ``x1'', I want ``alphabet'',
and ``x2'' should become ``zoology.'' The quotes in the R code
indicate that alphabet and zoology are character strings, not other
objects that already exist. Consider replacing x1 and x2 in x1 + x2
+ log(x1) + x3:

<<>>=
substitute(expression(x1 + x2 + log(x1) + x3), sublist)
@

Note that the substitution 1) leaves other variables alone (since
they are not named in sublist) and 2) it finds all valid uses of the
symbols x1 and x2 and replaces them.

This isn't quite what I wanted, however, because the strings have
been inserted into the middle of my expression. I don't want text.
I just want symbols. I've seen the conversion from character to symbol
done with as.symbol() and with as.name(), recently I realized that
they are synonymous. I usually use as.symbol() because that name has
more intuition for me, but in Gabor's answer to my question, as.name()
is used. Using as.name(), my example would become:

<<>>=
sublist <- list(x1 = as.name("alphabet"), x2 = as.name("zoology"))
substitute(expression(x1 + x2 + log(x1) + x3), sublist)
@


\subsection{setNames and names}

Almost every R user has noticed that the elements of R lists can have
names. In a data frame, the names of the list elements are thought
of as variable names, or column names. If dat is a data frame, the
names and colnames functions return the same thing, but that's not
true for other types of objects.

<<>>=
dat <- data.frame(x1=1:10, x2=10:1, x3=rep(1:5,2), x4=gl(2,5))
colnames(dat)
names(dat)
@

After dat is created, we can change the names inside it with a very
similar approach:

<<>>=
newnames <- c("whatever","sounds","good","tome")
colnames(dat) <- newnames
colnames(dat)
@

While used interactively, this is convenient, but it is a bit tedious
because we have to create dat first, and then set the names. The setNames()
function allows us to do this in one shot. I'll paste the data frame
creating commands and the name vector in for a first try:

<<>>=
dat2 <- setNames(data.frame(x1 = rnorm(10), x2 = rnorm(10),
    x3 = rnorm(10), x4 = gl(2,5)), c("good", "names", "tough", "find"))
head(dat2, 2)
@

In order to make this more generally useful, the first step is to
take the data-frame-creating code and set it into an expression that
is not immediately evaluated (that's datcommand in what follows).
When I want the data frame to be created, I use eval(), and then the
newnames vector is put to use. 

<<>>=
newnames <- c("iVar", "uVar", "heVar", "sheVar")
datcommand <- expression(data.frame(x1=1:10, x2=10:1, x3=rep(1:5,2), x4=gl(2,5)))
eval(datcommand)
dat3 <- setNames(eval(datcommand), newnames)
@

The whole point of this exercise is that we can write code that creates
the names, and creates the data frame, and then they all come together.

What if we have just one element in a list? In Gabor's answer to my
question, there is this idiom

\inputencoding{latin9}\begin{lstlisting}
setNames(list(as.name("x1c")), "x1")))
\end{lstlisting}
\inputencoding{utf8}

Consider this from the inside out. 
\begin{enumerate}
\item as.name(``x1c'') is an R symbol object,
\item list(as.name(``x1c'')) is a list with just one object, which is
that symbol object. 
\item Use setNames(). The object has no name! We would like to name it ``x1''. 
\end{enumerate}
It is as if we had run the command list(x1 = x1c). The big difference,
of course, is that this way is much more flexible because we can calculate
replacements. 


\subsection{The Big Finish}

In the meanCenter() function in rockchalk, some predictors are mean-centered
and their names are revised. A variable named ``age'' becomes ``agec''
or ``x1'' becomes ``x1c''. So the user's regression formula that
uses variables age or x1 must be revised. This is a function that
takes a formula ``fmla'' and replaces a symbol xname with newname.

\inputencoding{latin9}\begin{lstlisting}
formulaReplace <- function(fmla, xname, newname){ 
    do.call("substitute", list(fmla, setNames(list(as.name(newname)), xname))) 
}
\end{lstlisting}
\inputencoding{utf8}

This is put to use in meanCenter(). Suppose a vector of variable names
called nc (stands for ``needs centering'') has already been calculated.
The function std() creates a centered variable.

\inputencoding{latin9}\begin{lstlisting}
newFmla <- mc$formula
for (i in seq_along(nc)){
    icenter <- std(stddat[, nc[i]])
    newname <- paste(as.character(nc[i]), "c", sep = "")
    newFmla <- formulaReplace(newFmla,  as.character(nc[i]), newname)
    nc[i] <- newname
}
\end{lstlisting}
\inputencoding{utf8}

If one has a copy of rockchalk 1.6 or newer, the evidence of the success
of this approach should be evident in the output of the command example(meanCenter). 


\section{Make Re-Usable Tools (Rather than Cutting and Pasting)}

We seek concise solutions that are generalizable. It is almost never
right to cut and paste and then make minor edits in each copy to achieve
particular purposes. This advice goes against the grain of the graduate
students that I work with. They will almost always use cut and paste
solutions. 

Here is an example of the problem. A person needed ``dummy variables''.
The code had several pages like this:

\inputencoding{latin9}\begin{lstlisting}
  if(setcorr == 1){
        corr.10 <-1
        corr.20 <-0
        corr.30 <-0
        corr.40 <-0
        corr.50 <-0
        corr.60 <-0
        corr.70 <-0
        corr.80 <-0
    }
    if(setcorr == 2){
        corr.10 <-1
        corr.20 <-1
        corr.30 <-0
        corr.40 <-0
        corr.50 <-0
        corr.60 <-0
        corr.70 <-0
        corr.80 <-0
    }
    if(setcorr == 3){
        corr.10 <-1
        corr.20 <-1
        corr.30 <-1
        corr.40 <-0
        corr.50 <-0
        corr.60 <-0
        corr.70 <-0
        corr.80 <-0
    }
\end{lstlisting}
\inputencoding{utf8}

Well, that's understandable, but a little bit embarrassing. I asked
``why do you declare these separate variables, why not make a vector?''
and ``can't you see a more succinct way to declare those things?''
The answer is ``we do it that way in SAS'' or ``this runs''. One
might suspect that the author was eager to say, ``my project is based
on 10,000 lines of R code'' in order to impress a lay audience.

If the mission is to create a vector with a certain number of 1's
at the beginning, surely either of these functions would be better:

<<>>=
biVec1 <- function(n = 3, n1s = 1) {
    c(rep(1, n1s), rep(0, n - n1s))
}

biVec2 <- function(n = 3, n1s = 1){
    x <- numeric(length = n)
    x[1:n1s] <- 1
    x
}

(corr <- biVec1(n = 8, n1s = 3))
(corr <- biVec2(n = 8, n1s = 3))
@

The cut and paste way is not wrong, exactly. Its tedious. 

I think any reasonable user should want a vector, rather than the
separate variables. But suppose the user is determined, and really
wants the individual variables named corr.10, corr.20, and so forth.
We can re-design this so that those variables will be sitting out
in the workspace after the function is run. Read help(``assign'')
and try

<<>>=
biVec3 <- function(n = 3, n1s = 1) {
    X <- c(rep(1, n1s), rep(0, n - n1s))
    xnam <- paste("corr.", 1:8, "0", sep = "")
    for(i in 1:n) assign(xnam[i], X[i], envir = .GlobalEnv) 
}
ls()
biVec3(n = 8, n1s = 3)
ls()
@

I don't think most people will actually want that kind of result,
but if they do, there is a way to get it.

If we want to turn biVec1() or biVec2() into general purpose functions,
we need to start thinking about the problem of unexpected user input.
Notice what happens if we ask for more 1's than the result vector
is supposed to contain:

\inputencoding{latin9}\begin{lstlisting}
> biVec1(n = 3, n1s = 7)
Error in rep(0, n - n1s) (from #2) : invalid 'times' argument
> biVec2(n = 3, n1s = 7)
[1] 1 1 1 1 1 1 1
\end{lstlisting}
\inputencoding{utf8}

Perhaps I prefer biVec1() because it throws an error when there is
unreasonable input, while biVec2() returns a longer vector than expected
with no error. 

If the user mistakenly enters non-integers, neither function generates
an error, and they give us unexpected output.

<<>>=
biVec1(3.3, 1.8)
biVec2(3.3, 1.8)
@

To my surprise, it is not an easy thing to ask a number if it is a
whole number (see help(``is.integer'')). For that, we introduce
a new function, is.wholenumber(), and put it to use in our new and
improved function

<<>>=
is.wholenumber <- function(x, tol = .Machine$double.eps^0.5){
 abs(x - round(x)) < tol
}

biVec1 <- function(n = 3, n1s = 1) {
    if(!(is.wholenumber(n) & is.wholenumber(n1s)))
        stop("Both n and n1s must be whole numbers (integers)")
    if(n1s > n)
        stop("n must be greater than or equal to n1s")
    c(rep(1, n1s), rep(0, n - n1s))
}
@

In all of the test cases I've tried, that works, although the real
numbers 7.0 and 4.0 are able to masquerade as integers.

\inputencoding{latin9}\begin{lstlisting}
> biVec1(3, 7)
Error in biVec1(3, 7) (from #4) : n must be greater than or equal to n1s
> biVec1(7, 3)
[1] 1 1 1 0 0 0 0
> biVec1(3.3, 4.4)
Error in biVec1(3.3, 4.4) (from #2) : 
  Both n and n1s must be whole numbers (integers)
> biVec1(7.0, 4.0)
[1] 1 1 1 1 0 0 0
> biVec1(7.0, 4.0)
[1] 1 1 1 1 0 0 0
\end{lstlisting}
\inputencoding{utf8}


\section{Function Arguments.}

While developing functions for the rockchalk package, one of the most
important learning experiences I've had is in argument management. 

To state the obvious, argument names should be clear, unambiguous,
convenient, short, and minimally necessary. Some people are gifted
in the selection of function and argument names, while others have
to try harder. 

I think these are the most important points.
\begin{enumerate}
\item Protect your function's calculations from the user's workspace.
\item Protect your function from user errors.
\item If possible, design your function so that it runs with a minimum number
of arguments.
\end{enumerate}
One of the reasons that it is difficult to design functions is that
there is no ``mandatory manual'' on this. The R framework allows
programmer creativity, which is a good thing. But it is also a bad
thing, since every programmer has his/her own opinion about what ought
to be done. 

Before I consider the 4 main points, I need to mention some R functions
that are very helpful in dealing with arguments.
\begin{description}
\item [{missing()}] Inside a function, we often wonder ``did the user
supply this argument in their function call?''
\item [{stop()}] Suppose we want the function to fail if something happens.
Write 


\inputencoding{latin9}\begin{lstlisting}
if (isTRUE(x)) stop(paste("I'm sorry, you failed to supply the right information. The variable x has value", x, "which is silly, from my point of view.")))
\end{lstlisting}
\inputencoding{utf8}

\item [{stopifnot()}] It is so frequent to run ``if(isTRUE(x)) stop()''
that they created a wrapper, stopifnot(x).
\item [{warning()}] If you don't want the function to fail, but you need
to warn the user about a condition that seems dubious, run


\inputencoding{latin9}\begin{lstlisting}
if (isTRUE(x)) warning(paste("Are you sure the value of x ought to be", x, "?"))
\end{lstlisting}
\inputencoding{utf8}

\end{description}

\subsection{Protect your function's calculations from the user's workspace.}

If I'm writing a little project for myself, I might be lazy and allow
R's ``lexical scoping'' process to find variables that are not defined
inside a function.

\inputencoding{latin9}\begin{lstlisting}
x <- c(1, 2, 3, 4)
y <- c(8, 9, 10, 11)
z <- c(1, 1, 2, 2)
plotme <- function(x){
    plot(x, y, col = z)
}
plotme(x)
\end{lstlisting}
\inputencoding{utf8}

I might write code for myself that lets the function go and find \code{y}
and \code{z}. This will run, but you have to admit it is pretty risky.
It depends on R's willingness to reach outside the boundaries of the
function to find \code{x}, \code{y}, and \code{z}. If I have inadvertently
altered \code{y}, this will end badly.

If I'm writing a function for somebody else to use, as in a package,
I believe the function should to NOT rely on variables from the environment.
In other words, I want to prevent R from filling in the gaps for users.
The reason is simple. Users may alter variables in their environment
and the function may behave badly as a result. The best case scenario
is that \code{y} and \code{z} do not exist in the workspace, so
the user will receive an error message:

\begin{Sinput}
> plotme(x)
\end{Sinput}
\begin{Schunk}
   \begin{Soutput}
Error in xy.coords(x, y, xlabel, ylabel, log) : object 'y' not found
  \end{Soutput}
\end{Schunk}

The worst case scenario is that the user has some other variables
y and z sitting about in the workspace. Those ``found'' variables
are put to use with a completely unintentional result. 

As a first step toward fixing this, I propose the following idea.
Every variable put to use inside a function should be named in the
list of arguments. If we define the function as follows,

\inputencoding{latin9}\begin{lstlisting}
plotme <- function(x, y, z){
    plot(x, y, col = z)
}
\end{lstlisting}
\inputencoding{utf8}

the user's use of \code{plotme(x)} will result in the error

\begin{Schunk}
   \begin{Soutput}
Error in xy.coords(x, y, xlabel, ylabel, log) :
    argument "y" is missing, with no default
   \end{Soutput}
\end{Schunk}Naming all of the arguments effectively protects the function against
the accidental importation of data. The major challenge is avoiding
the user error of accidentally depending on a variable that is in
the workspace (the global environment). I often make this mistake,
that's why I'm bringing it to your attention. 

There's a function called checkUsage() in the codetools package that
can help check functions for reliance on global variables. First,
we'll make sure the workspace is clean by removing \code{x}, \code{y},
and \code{z}.

\begin{Schunk}
   \begin{Soutput}
> rm(x, y, z)
> library(codetools) 
> checkUsage(plotme) 
<anonymous>: no visible binding for global variable 'y' (:2) 
<anonymous>: no visible binding for global variable 'z' (:2)
   \end{Soutput}
\end{Schunk}

Note that, if the variables x, y, an z exist in the user workspace,
they are global and checkUsage will not offer a warning about them.
Hence, it is very important to run checkUsage() after cleaning up
the workspace. 

checkUsage() will also offer warnings about unused variables. This
can be helpful in cleaning up a function's variables.


\subsection{Protect your function from user errors.}

Beyond insisting that all variables that depend on input from the
workspace must be explicitly named, we might be interested to know
if the user supplied reasonable arguments. Suppose our plotme function
can work well if the user supplies x and y, and we will manufacture
z for them if it is not supplied. I've used this approach in the past.

<<eval=F>>=
plotme <- function(x, y, z){
    if (missing(z)) z <- rep(2, length(x))   
    plot(x, y, col = z)
}
@

Note it would have the same effect to supply that value of z as a
default, as in

<<eval=F>>=
plotme <- function(x, y, z = rep(2, length(x))){
    plot(x, y, col = z)
}
@

Some people prefer this more elaborate declaration. This succeeds
because the variables that are declared in the beginning of the list
are available for calculations in the later part of the list.

That will often be sufficient, but we have not protected ourselves
as well as we might. Suppose the user's code ends up supplying a NULL
value for \code{z}, as in \code{plotme(x, y, z = NULL)}. They would
not do that on purpose, but it might be an accident. If \code{z}
is calculated in their code and something has gone wrong. To protect
against that, we find this idiom is used in the R source code quite
often:

<<eval=F>>=
plotme <- function(x, y, z){
    if (missing(z) | is.null(z)) z <- rep(2, length(x))   
    plot(x, y, col = z)
}
@

If \code{z} is either not supplied by the user's command, or if the
user's command explicitly sets it to NULL, then we'll fabricate \code{z}.

Sometimes even more elaborate argument checking will be required.
We can explicitly check if z is not of the correct type (integer)
or length.

<<eval=F>>=
plotme <- function(x, y, z){
    if (missing(z) | is.null(z)) {
        z <- rep(2, length(x))
    } else {
        if (!is.integer(z) | length(z) != length(x)) stop("z is inappropriate")
	}
    plot(x, y, col = z)
}
@

After that, an incorrect argument, such as z = ``fred'' or z = c(1,
2, 2), will result in an error (which is what we want).

\begin{Sinput}
> plotme(x, y, z = "fred")
\end{Sinput}
\begin{Soutput}
Error in plotme(x, y, z = c(1, 2, 2)) : z is inappropriate
\end{Soutput}


\subsection{If possible, design your function so that it runs with a minimum
number of arguments.}

Take a look at the most commonly used functions in R, such as lm()
or plot.default(). These functions allow a great many arguments, but
they will deliver what the users minimally need if the users supply
a formula and a data frame.

I have made many mistakes in designing function in rockchalk. From
one month to the next, I would learn ways to simplify arguments or
eliminate them entirely.

I did not initially understand the flexibility that R allows in the
types of arguments passed to a function. Because we can deduce information
from the arguments that are passed in, we can eliminate the need for
the user to pass in other arguments. We would never need to require
both arguments x and xlab in the plot function, for example. xlab
is optional because, In particular, I insisted that users supply quoted
strings as variable names for the arguments plotx and modx. I insisted
on this syntax, as illustrated in the example for plotSlopes:

<<include=F>>=
library(rockchalk)
@

<<eval=F>>=
m7 <- lm(statusquo ~ region * income, data= Chile)
plotSlopes(m7, plotx = "income", modx = "region", interval = "conf")
@

Today, the following syntax works just as well

<<eval=F>>=
m7 <- lm(statusquo ~ region * income, data= Chile)
plotSlopes(m7, plotx = income, modx = region, interval = "conf")
@

It is no longer necessary to quote the income and region variable
because the function checks the argument and, if necessary, it manufactures
the string for the user. This is what I have in the beginning of plotSlopes.

<<eval=F>>=
plotx <- as.character(substitute(plotx))[1L]
if (!missing(modx)) {
    modx <- as.character(substitute(modx))[1L]
}
@

If plotx and modx are already character strings, then this has no
effect. If they are symbols, however, this turns them into character
strings.

In the olden days,when I first started using R, it was necessary to
explicitly quite many arguments that we do not quite anymore. Today,
for example, both library(``rockchalk'') and library(rockchalk)
will work. The latter did not work in the beginning, however. If you
go looking at old R code, from 1999 or 2000, you will find many more
variables are quoted. 

This idiom is widely used in the R source code to eliminate the user
tedium of typing quotation marks. Note that today, these commands
have the same effect:

\begin{Sinput}
methods(plot)
\end{Sinput}

or this

\begin{Sinput}
methods(plot)
\end{Sinput}

Why does the latter work, when at one time the former was need? Check
the source code of the methods() function

<<eval=F>>=
if (!is.character(generic.function)) generic.function <- deparse(substitute(generic.function))
@This trick is put to use in the plot.default() function in R, where
the user can supply an argument xlab or ylab if desired, but the plot.default
will create those values if they are not supplied.


\section{Do This, Not That (Stub)}

R novices sometimes use Google to search for R advice and they find
it, good or bad. They may find their way to the r-help email list,
where advice is generally good, or to the StackOverflow pages for
R, which may be better. A lot of advice is offered by people like
me, who may have good intentions, but are simply not qualified to
offer advice. 

One of the few bits of advice that seems to grab widespread support
is that ``for loops are bad.'' One can write an lapply statement
in one line, while a for loop can take 3 lines. The code is shorter,
but it won't necessarily run more quickly. I recall being jarred by
this revelation in John Chambers's book, \emph{Software for Data Analysis}.
The members of the apply family (apply, lapply, sapply, etc) can make
for more readable code, but they aren't always faster. ``However,
none of the apply mechanisms changes the number of times the supplied
function is called, so serious improvements will be limited to iterating
simple calculations many times. Otherwise, the n evaluations of the
function can be expected to be the dominant fraction of the computation''\citep[213]{chambers_software_2008}.

Todo: insert discussion of stackListItems-001.

Insert alternative methods of measuring executation time and measuring
performance

Balance time spent optimizing code versus time spent running program.


\section{Suggested Chores}

I suggest the would be programmer should take on some basic challenges. 
\begin{enumerate}
\item Try to create a generic function and several methods to handle classes
of various types. If the term ``generic function'' and ``method''
cause disorientation, that means the reader is a user, not a programmer
yet. 
\item Consider developing a routine (or package) for statistical estimation
or presentation. Find several R packages and compare the R code in
them. Let me know if you agree with me about these points.\end{enumerate}
\begin{itemize}
\item Good code is compartmentalized. Separate pieces of work are handled
by separated functions and results are returned in a well organized
way.
\item Functions should not be HUGE. 


The components of a project should be small enough so that we can
comprehend them. Generally speaking, if we are reading code and we
come to a line that uses a variable that we cannot find on the screen,
that's a problem. I used to correspond with a programmer at the University
of Michigan who said he began to feel uncomfortable when a function
filled up the entire terminal screen. 

\item People who copy and paste sections over and over in order to handle
slightly different cases are causing trouble for themselves and others.
They should think harder on ways to separate that work into re-usable
functions. 
\end{itemize}
\bibliographystyle{chicago}
\bibliography{rockchalk}

\end{document}
